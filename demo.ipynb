{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"7\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm.autonotebook import tqdm\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from glob import glob\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms as T\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision\n",
    "import tifffile as tiff\n",
    "from torchvision import transforms\n",
    "from tqdm.auto import tqdm\n",
    "from torchmetrics.classification import BinaryJaccardIndex\n",
    "from torchmetrics.classification import BinaryF1Score\n",
    "from torchmetrics.classification import BinaryPrecision\n",
    "from torchmetrics.classification import BinaryRecall\n",
    "\n",
    "\n",
    "import joblib\n",
    "import torch.nn.functional as F\n",
    "\n",
    "seed = 20240308"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently using \"cuda\" device.\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f'Currently using \"{device}\" device.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)               # Python의 내장 난수 생성기\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)            # Numpy의 난수 생성기\n",
    "    torch.manual_seed(seed)         # PyTorch의 난수 생성기\n",
    "    torch.cuda.manual_seed(seed)    # CUDA의 난수 생성기\n",
    "    torch.cuda.manual_seed_all(seed) # 멀티-GPU 환경에서 CUDA 모든 난수 생성기\n",
    "\n",
    "    # PyTorch가 가능한 한 결정적으로 동작하도록 하는 몇 가지 설정\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "seed_everything(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 6\n",
    "image_size = 256\n",
    "num_classes = 1\n",
    "epochs = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>images</th>\n",
       "      <th>masks</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/home/hyj/ChanHyung/Image_segementation/Forest...</td>\n",
       "      <td>/home/hyj/ChanHyung/Image_segementation/Forest...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/home/hyj/ChanHyung/Image_segementation/Forest...</td>\n",
       "      <td>/home/hyj/ChanHyung/Image_segementation/Forest...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              images  \\\n",
       "0  /home/hyj/ChanHyung/Image_segementation/Forest...   \n",
       "1  /home/hyj/ChanHyung/Image_segementation/Forest...   \n",
       "\n",
       "                                               masks  \n",
       "0  /home/hyj/ChanHyung/Image_segementation/Forest...  \n",
       "1  /home/hyj/ChanHyung/Image_segementation/Forest...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_images = '/home/hyj/ChanHyung/Image_segementation/Forest_Fire_Segmentation/dataset/train_img/'\n",
    "path_masks = '/home/hyj/ChanHyung/Image_segementation/Forest_Fire_Segmentation/dataset/train_mask/'\n",
    "\n",
    "images_paths = glob(path_images + '*.tif')\n",
    "masks_paths = glob(path_masks + '*.tif')\n",
    "\n",
    "images_paths = sorted([str(p) for p in images_paths])\n",
    "masks_paths = sorted([str(p) for p in masks_paths])\n",
    "\n",
    "df_train = pd.DataFrame({'images': images_paths, 'masks': masks_paths})\n",
    "\n",
    "df_train.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>images</th>\n",
       "      <th>masks</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/home/hyj/ChanHyung/Image_segementation/Forest...</td>\n",
       "      <td>/home/hyj/ChanHyung/Image_segementation/Forest...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/home/hyj/ChanHyung/Image_segementation/Forest...</td>\n",
       "      <td>/home/hyj/ChanHyung/Image_segementation/Forest...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              images  \\\n",
       "0  /home/hyj/ChanHyung/Image_segementation/Forest...   \n",
       "1  /home/hyj/ChanHyung/Image_segementation/Forest...   \n",
       "\n",
       "                                               masks  \n",
       "0  /home/hyj/ChanHyung/Image_segementation/Forest...  \n",
       "1  /home/hyj/ChanHyung/Image_segementation/Forest...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_images = '/home/hyj/ChanHyung/Image_segementation/Forest_Fire_Segmentation/dataset/test_img/'\n",
    "path_masks = '/home/hyj/ChanHyung/Image_segementation/Forest_Fire_Segmentation/dataset/test_mask/'\n",
    "\n",
    "images_paths = glob(path_images + '*.tif')\n",
    "masks_paths = glob(path_masks + '*.tif')\n",
    "\n",
    "images_paths = sorted([str(p) for p in images_paths])\n",
    "masks_paths = sorted([str(p) for p in masks_paths])\n",
    "\n",
    "df_test = pd.DataFrame({'images': images_paths, 'masks': masks_paths})\n",
    "\n",
    "df_test.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, dataframe, train_mode=True, transform=None):\n",
    "        self.dataframe = dataframe\n",
    "        self.train_mode = train_mode\n",
    "        self.transform = transform\n",
    "\n",
    "        self.MAX_PIXEL_VALUE = 65535 \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.dataframe.iloc[idx, 0]\n",
    "        image = tiff.imread(image_path).astype('float32')\n",
    "\n",
    "        # AFI 채널 계산\n",
    "        AFI_channel = image[:, :, 6] / np.maximum(image[:, :, 1], 1)  # 0으로 나누는 것을 방지\n",
    "        AFI_channel = np.expand_dims(AFI_channel, axis=2)\n",
    "\n",
    "        # 원하는 채널 선택 및 AFI 채널 추가\n",
    "        selected_channels = image[:, :, [0,1,2,3,4,5,6,8,9]]  # Blue, SWIR1, SWIR2 선택\n",
    "        image_with_AFI = np.concatenate((selected_channels, AFI_channel), axis=2)  # AFI 채널 추가\n",
    "\n",
    "        # 이미지 정규화 및 텐서 변환\n",
    "        image_with_AFI = torch.from_numpy(image_with_AFI).permute(2, 0, 1) / self.MAX_PIXEL_VALUE\n",
    "\n",
    "        if self.train_mode or len(self.dataframe.columns) == 2:\n",
    "            mask_path = self.dataframe.iloc[idx, 1]\n",
    "            mask = tiff.imread(mask_path).astype('float32')\n",
    "            mask = torch.from_numpy(mask).unsqueeze(0)  # 채널 차원 추가\n",
    "\n",
    "            return image_with_AFI, mask\n",
    "        else:\n",
    "            return image_with_AFI\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CustomDataset(df_train)\n",
    "valid_dataset = CustomDataset(df_test, train_mode=False)\n",
    "# test_dataset = CustomDataset(test, train_mode=False)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "# test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class InceptionConv(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, mid_channels=None):\n",
    "        super().__init__()\n",
    "        if not mid_channels:\n",
    "            mid_channels = out_channels\n",
    "\n",
    "        self.double_conv1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, mid_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(mid_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(mid_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "        self.double_conv2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, mid_channels, kernel_size=5, padding=2, bias=False),\n",
    "            nn.BatchNorm2d(mid_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(mid_channels, out_channels, kernel_size=5, padding=2, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "        self.double_conv3 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, mid_channels, kernel_size=1, padding=0, bias=False),\n",
    "            nn.BatchNorm2d(mid_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "        self.double_conv4 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, mid_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(mid_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(mid_channels, out_channels, kernel_size=1, padding=0, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.conv_Inc = nn.Sequential(\n",
    "            nn.Conv2d(4 * out_channels, out_channels, kernel_size=1, stride=1, padding=0, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "        )\n",
    "        self.conv_skip = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "        )\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        outputs = [self.double_conv1(x), self.double_conv2(x), self.double_conv3(x), self.double_conv4(x)]\n",
    "        output2 = self.conv_Inc(torch.cat(outputs, 1))\n",
    "        xx = output2 + self.conv_skip(x)\n",
    "        xx_o = self.relu(xx)\n",
    "        return xx_o\n",
    "\n",
    "\n",
    "class UpSample(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, up_sample_mode):\n",
    "        super(UpSample, self).__init__()\n",
    "        if up_sample_mode == 'conv_transpose':\n",
    "            self.up_sample = nn.ConvTranspose2d(in_channels, out_channels, kernel_size=2, stride=2)\n",
    "        elif up_sample_mode == 'bilinear':\n",
    "            self.up_sample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported `up_sample_mode` (can take one of `conv_transpose` or `bilinear`)\")\n",
    "\n",
    "    def forward(self, down_input):\n",
    "        x = self.up_sample(down_input)\n",
    "        return x\n",
    "\n",
    "\n",
    "class UnetGridGatingSignal(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, stride, padding):\n",
    "        super(UnetGridGatingSignal, self).__init__()\n",
    "\n",
    "        self.conv_block = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                input_dim, output_dim, kernel_size=1, stride=stride, padding=padding\n",
    "            ),\n",
    "            nn.BatchNorm2d(output_dim),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        xx = self.conv_block(x)\n",
    "        return xx\n",
    "\n",
    "\n",
    "class AttentionBlock(nn.Module):\n",
    "    \"\"\"Attention block with learnable parameters\"\"\"\n",
    "\n",
    "    def __init__(self, F_g, F_l, n_coefficients):\n",
    "        \"\"\"\n",
    "        :param F_g: number of feature maps (channels) in previous layer\n",
    "        :param F_l: number of feature maps in corresponding encoder layer, transferred via skip connection\n",
    "        :param n_coefficients: number of learnable multi-dimensional attention coefficients\n",
    "        \"\"\"\n",
    "        super(AttentionBlock, self).__init__()\n",
    "\n",
    "        self.W_gate = nn.Sequential(\n",
    "            nn.Conv2d(F_g, n_coefficients, kernel_size=1, stride=1, padding=0),\n",
    "            nn.BatchNorm2d(n_coefficients)\n",
    "        )\n",
    "\n",
    "        self.W_x = nn.Sequential(\n",
    "            nn.Conv2d(F_l, n_coefficients, kernel_size=1, stride=2, padding=0),\n",
    "            nn.BatchNorm2d(n_coefficients)\n",
    "        )\n",
    "\n",
    "        self.psi = nn.Sequential(\n",
    "            nn.Conv2d(n_coefficients, 1, kernel_size=1, stride=1, padding=0),\n",
    "            nn.BatchNorm2d(1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        self.UpSampling2D = nn.Upsample(scale_factor=2)\n",
    "\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(n_coefficients, n_coefficients, kernel_size=1, stride=1, padding=0),\n",
    "            nn.BatchNorm2d(n_coefficients)\n",
    "        )\n",
    "\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, gate, skip_connection):\n",
    "        \"\"\"\n",
    "        :param gate: gating signal from previous layer\n",
    "        :param skip_connection: activation from corresponding encoder layer\n",
    "        :return: output activations\n",
    "        \"\"\"\n",
    "\n",
    "        g1 = self.W_gate(gate)\n",
    "        x1 = self.W_x(skip_connection)\n",
    "        psi = self.relu(g1 + x1)\n",
    "        psi = self.psi(psi)\n",
    "        upsample_sigmoid_xg = self.UpSampling2D(psi)\n",
    "        out = skip_connection * upsample_sigmoid_xg.expand_as(skip_connection)\n",
    "        return out\n",
    "\n",
    "\n",
    "class base_Unet(nn.Module):\n",
    "    def __init__(self, img_ch=10, output_ch=1, filters=[16, 32, 64, 128, 256, 512]):\n",
    "        super(base_Unet, self).__init__()\n",
    "\n",
    "        self.MaxPool = nn.MaxPool2d(kernel_size=2, stride=None)\n",
    "        self.Dropout = nn.Dropout(p=0.1, inplace=True)\n",
    "\n",
    "        self.conv_1 = InceptionConv(img_ch, filters[0])\n",
    "        self.conv_2 = InceptionConv(filters[0], filters[1])\n",
    "        self.conv_3 = InceptionConv(filters[1], filters[2])\n",
    "        self.conv_4 = InceptionConv(filters[2], filters[3])\n",
    "        self.conv_5 = InceptionConv(filters[3], filters[4])\n",
    "        self.conv_6 = InceptionConv(filters[4], filters[5])\n",
    "\n",
    "        self.gating = UnetGridGatingSignal(filters[5], filters[4], 1, 0)\n",
    "\n",
    "        self.upsample1 = UpSample(filters[5], filters[5], up_sample_mode='bilinear')\n",
    "        self.Att1 = AttentionBlock(F_g=filters[5], F_l=filters[4], n_coefficients=filters[4])\n",
    "        self.up_conv1 = InceptionConv(filters[5] + filters[4], filters[4])\n",
    "\n",
    "        self.upsample2 = UpSample(filters[4], filters[4], up_sample_mode='bilinear')\n",
    "        self.Att2 = AttentionBlock(F_g=filters[4], F_l=filters[3], n_coefficients=filters[3])\n",
    "        self.up_conv2 = InceptionConv(filters[4] + filters[3], filters[3])\n",
    "\n",
    "        self.upsample3 = UpSample(filters[3], filters[3], up_sample_mode='bilinear')\n",
    "        self.Att3 = AttentionBlock(F_g=filters[3], F_l=filters[2], n_coefficients=filters[2])\n",
    "        self.up_conv3 = InceptionConv(filters[3] + filters[2], filters[2])\n",
    "\n",
    "        self.upsample4 = UpSample(filters[2], filters[2], up_sample_mode='bilinear')\n",
    "        self.Att4 = AttentionBlock(F_g=filters[2], F_l=filters[1], n_coefficients=filters[1])\n",
    "        self.up_conv4 = InceptionConv(filters[2] + filters[1], filters[1])\n",
    "\n",
    "        self.upsample5 = UpSample(filters[1], filters[1], up_sample_mode='bilinear')\n",
    "        self.Att5 = AttentionBlock(F_g=filters[1], F_l=filters[0], n_coefficients=filters[0])\n",
    "        self.up_conv5 = InceptionConv(filters[1] + filters[0], filters[0])\n",
    "\n",
    "        self.output_layer = nn.Conv2d(filters[0], output_ch, 1, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encode\n",
    "        x1 = self.conv_1(x)\n",
    "        e1 = self.MaxPool(x1)\n",
    "\n",
    "        x2 = self.conv_2(e1)\n",
    "        e2 = self.MaxPool(x2)\n",
    "\n",
    "        x3 = self.conv_3(e2)\n",
    "        e3 = self.MaxPool(x3)\n",
    "\n",
    "        x4 = self.conv_4(e3)\n",
    "        e4 = self.MaxPool(x4)\n",
    "\n",
    "        x5 = self.conv_5(e4)\n",
    "        e5 = self.MaxPool(x5)\n",
    "\n",
    "        x6 = self.conv_6(e5)\n",
    "\n",
    "        # Decode\n",
    "        x66 = self.upsample1(x6)\n",
    "        g_conv5 = self.Att1(x6, x5)\n",
    "        x7 = torch.cat((g_conv5, x66), dim=1)\n",
    "        x8 = self.up_conv1(x7)\n",
    "\n",
    "        x88 = self.upsample2(x8)\n",
    "        g_conv4 = self.Att2(x8, x4)\n",
    "        x9 = torch.cat((g_conv4, x88), dim=1)\n",
    "        x10 = self.up_conv2(x9)\n",
    "\n",
    "        x1010 = self.upsample3(x10)\n",
    "        g_conv3 = self.Att3(x10, x3)\n",
    "        x11 = torch.cat((g_conv3, x1010), dim=1)\n",
    "        x12 = self.up_conv3(x11)\n",
    "\n",
    "        x1212 = self.upsample4(x12)\n",
    "        g_conv2 = self.Att4(x12, x2)\n",
    "        x13 = torch.cat((g_conv2, x1212), dim=1)\n",
    "        x14 = self.up_conv4(x13)\n",
    "\n",
    "        x1414 = self.upsample5(x14)\n",
    "        g_conv1 = self.Att5(x14, x1)\n",
    "        x15 = torch.cat((g_conv1, x1414), dim=1)\n",
    "        x16 = self.up_conv5(x15)\n",
    "\n",
    "        output = self.output_layer(x16)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = base_Unet().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Lovasz-Softmax and Jaccard hinge loss in PyTorch\n",
    "Maxim Berman 2018 ESAT-PSI KU Leuven (MIT License)\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "def lovasz_grad(gt_sorted):\n",
    "    \"\"\"\n",
    "    Computes gradient of the Lovasz extension w.r.t sorted errors\n",
    "    See Alg. 1 in paper\n",
    "    \"\"\"\n",
    "    p = len(gt_sorted)\n",
    "    gts = gt_sorted.sum()\n",
    "    intersection = gts - gt_sorted.float().cumsum(0)\n",
    "    union = gts + (1 - gt_sorted).float().cumsum(0)\n",
    "    jaccard = 1 - intersection / union\n",
    "    if p > 1:  # cover 1-pixel case\n",
    "        jaccard[1:p] = jaccard[1:p] - jaccard[0:-1]\n",
    "    return jaccard\n",
    "\n",
    "\n",
    "def hinge(pred, label):\n",
    "    signs = 2 * label - 1\n",
    "    errors = 1 - pred * signs\n",
    "    return errors\n",
    "\n",
    "\n",
    "def lovasz_hinge_flat(logits, labels, ignore_index):\n",
    "    \"\"\"\n",
    "    Binary Lovasz hinge loss\n",
    "      logits: [P] Variable, logits at each prediction (between -\\infty and +\\infty)\n",
    "      labels: [P] Tensor, binary ground truth labels (0 or 1)\n",
    "      ignore_index: label to ignore\n",
    "    \"\"\"\n",
    "    logits = logits.contiguous().view(-1)\n",
    "    labels = labels.contiguous().view(-1)\n",
    "    if ignore_index is not None:\n",
    "        mask = labels != ignore_index\n",
    "        logits = logits[mask]\n",
    "        labels = labels[mask]\n",
    "    errors = hinge(logits, labels)\n",
    "    errors_sorted, perm = torch.sort(errors, dim=0, descending=True)\n",
    "    perm = perm.data\n",
    "    gt_sorted = labels[perm]\n",
    "    grad = lovasz_grad(gt_sorted)\n",
    "    loss = torch.dot(F.elu(errors_sorted) + 1, grad)\n",
    "    return loss\n",
    "    \n",
    "def lovasz_hinge(logits, labels, per_image=True, ignore=None):\n",
    "    \"\"\"\n",
    "    Binary Lovasz hinge loss\n",
    "      logits: [B, H, W] Variable, logits at each pixel (between -\\infty and +\\infty)\n",
    "      labels: [B, H, W] Tensor, binary ground truth masks (0 or 1)\n",
    "      per_image: compute the loss per image instead of per batch\n",
    "      ignore: void class id\n",
    "    \"\"\"\n",
    "    if per_image:\n",
    "        loss = mean(lovasz_hinge_flat(*flatten_binary_scores(log.unsqueeze(0), lab.unsqueeze(0), ignore))\n",
    "                          for log, lab in zip(logits, labels))\n",
    "    else:\n",
    "        loss = lovasz_hinge_flat(*flatten_binary_scores(logits, labels, ignore))\n",
    "    return loss\n",
    "\n",
    "class LovaszLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Binary Lovasz hinge loss\n",
    "      logits: [P] Variable, logits at each prediction (between -\\infty and +\\infty)\n",
    "      labels: [P] Tensor, binary ground truth labels (0 or 1)\n",
    "      ignore_index: label to ignore\n",
    "    \"\"\"\n",
    "    def __init__(self, ignore_index=None):\n",
    "        super().__init__()\n",
    "        self.ignore_index = ignore_index\n",
    "\n",
    "    def forward(self, logits, labels):\n",
    "        return lovasz_hinge_flat(logits, labels, self.ignore_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, epoch, dataloader, optimizer, criterion, f1_score_metric, miou_metric):\n",
    "    model.train()\n",
    "    train_loss = []\n",
    "    \n",
    "    progress_bar = tqdm(dataloader, desc=f\"Epoch {epoch}/{epochs} Training\")\n",
    "    for imgs, labels in progress_bar:\n",
    "        imgs, labels = imgs.float().to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        output = model(imgs)\n",
    "        loss = criterion(output, labels)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss.append(loss.item())\n",
    "\n",
    "        # F1 Score와 MIoU 계산\n",
    "        f1_score = f1_score_metric(output, labels)\n",
    "        miou = miou_metric(output, labels)\n",
    "\n",
    "        # tqdm 바 업데이트 (손실, F1 Score, MIoU 포함)\n",
    "        progress_bar.set_postfix(loss=loss.item(), F1_score=f1_score.item(), mIoU=miou.item())\n",
    "\n",
    "    # epoch당 훈련 메트릭 평균 계산\n",
    "    avg_train_loss = sum(train_loss) / len(train_loss)\n",
    "    train_f1 = f1_score_metric.compute()\n",
    "    train_miou = miou_metric.compute()\n",
    "    \n",
    "    miou_metric.reset()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    return {\"train_loss\": avg_train_loss, \"train_f1\":train_f1, \"train_miou\":train_miou}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(model, dataloader, criterion, f1_score_metric, miou_metric):\n",
    "    model.eval()\n",
    "    val_loss = []\n",
    "    saved_outputs, saved_labels = [], []\n",
    "\n",
    "    f1_score_metric.reset()\n",
    "    miou_metric.reset()\n",
    "\n",
    "    progress_bar = tqdm(dataloader, desc=\"Valid\")\n",
    "    with torch.no_grad():\n",
    "        for imgs, labels in progress_bar:\n",
    "            imgs, labels = imgs.float().to(device), labels.to(device)\n",
    "\n",
    "            output = model(imgs)\n",
    "            loss = criterion(output, labels)\n",
    "\n",
    "\n",
    "            f1_score = f1_score_metric(output, labels)\n",
    "            miou = miou_metric(output, labels)\n",
    "\n",
    "            saved_outputs.append(output)\n",
    "            saved_labels.append(labels)\n",
    "\n",
    "            progress_bar.set_postfix(loss=loss.item(), F1_score=f1_score.item(), mIoU=miou.item())\n",
    "            val_loss.append(loss.item())\n",
    "\n",
    "    saved_outputs = torch.cat(saved_outputs, dim=0)\n",
    "    saved_labels = torch.cat(saved_labels, dim=0)\n",
    "    avg_val_loss = sum(val_loss) / len(val_loss)\n",
    "    val_f1 = f1_score_metric.compute()\n",
    "    val_miou = miou_metric.compute()\n",
    "    miou_metric.reset()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    return {\"saved_outputs\":saved_outputs, \"saved_labels\":saved_labels, \"val_loss\":avg_val_loss, \"val_f1\":val_f1, \"val_miou\":val_miou}\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visual(epoch, k, model, train_CFG, val_CFG, best_val_miou, threshold_min=-10, threshold_max=1, interv=0.1):\n",
    "    # epoch 결과 출력\n",
    "    print(f\"Epoch {epoch}/{epochs} - Train Loss: {train_CFG['train_loss']:.4f}, Train F1: {train_CFG['train_f1']:.4f}, Train MIoU: {train_CFG['train_miou']:.4f}, \"\n",
    "            f\"Val Loss: {val_CFG['val_loss']:.4f}, Val F1: {val_CFG['val_f1']:.4f}, Val MIoU: {val_CFG['val_miou']:.4f}\")\n",
    "\n",
    "    \n",
    "    miou_values = []\n",
    "    thresholds = np.arange(threshold_min, threshold_max, interv)\n",
    "    miou_tmp = BinaryJaccardIndex().to(device)\n",
    "\n",
    "    for threshold in tqdm(thresholds):\n",
    "        # output을 이진 마스크로 변환\n",
    "        miou_tmp.reset()\n",
    "        saved_outputs_binary = val_CFG['saved_outputs'] > threshold\n",
    "        miou = miou_tmp(saved_outputs_binary.cuda(), val_CFG['saved_labels'])\n",
    "        miou_values.append(miou.item())\n",
    "\n",
    "    # 가장 높은 mIoU 값 찾기\n",
    "    max_miou = max(miou_values)\n",
    "    max_threshold = thresholds[miou_values.index(max_miou)]\n",
    "\n",
    "    # 최적 모델 저장 로직 (예시)\n",
    "    if max_miou > best_val_miou:\n",
    "        best_val_miou = max_miou\n",
    "        torch.save(model, f\"./Fold_Model/{k}_{epoch}_{max_miou:.4f}_{max_threshold:.4f}.pth\")\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(thresholds, miou_values, marker='o', linestyle='-', color='b')\n",
    "    plt.title(f'max_threshold: {max_threshold}, max_MIoU: {max_miou}')\n",
    "    plt.xlabel('Threshold')\n",
    "    plt.ylabel('Mean Intersection over Union (MIoU)')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    return {\"best_val_miou\": best_val_miou}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = LovaszLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-3)  # if unfreeze=True -> 1e-4, 1e-5, so not to ruin good init w\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=1, eta_min=0.001, last_epoch=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torchmetrics를 사용하여 F1 Score와 MIOU 계산을 위한 준비\n",
    "f1_score_metric = BinaryF1Score().to(device)\n",
    "miou_metric = BinaryJaccardIndex(threshold=0.55).to(device)\n",
    "precision_metric = BinaryPrecision().to(device)\n",
    "recall_metric = BinaryRecall().to(device)\n",
    "miou_tmp = BinaryJaccardIndex().to(device)\n",
    "model.to(device)\n",
    "\n",
    "best_val_miou = 0.8158\n",
    "best_val_loss = 999999999999\n",
    "best_model = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_val_miou = 0.8158\n",
    "best_val_loss = 999999999999\n",
    "best_model = None\n",
    "for epoch in range(1, epochs + 1):\n",
    "    model.train()\n",
    "    train_loss = []\n",
    "    \n",
    "    progress_bar = tqdm(train_dataloader, desc=f\"Epoch {epoch}/{epochs} Training\")\n",
    "    for imgs, labels in progress_bar:\n",
    "        imgs, labels = imgs.float().to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        output = model(imgs)\n",
    "        loss = criterion(output, labels)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss.append(loss.item())\n",
    "\n",
    "        # F1 Score와 MIoU 계산\n",
    "        f1_score = f1_score_metric(output, labels)\n",
    "        miou = miou_metric(output, labels)\n",
    "\n",
    "        # tqdm 바 업데이트 (손실, F1 Score, MIoU 포함)\n",
    "        progress_bar.set_postfix(loss=loss.item(), F1_score=f1_score.item(), mIoU=miou.item())\n",
    "\n",
    "    # epoch당 훈련 메트릭 평균 계산\n",
    "    avg_train_loss = sum(train_loss) / len(train_loss)\n",
    "    train_f1 = f1_score_metric.compute()\n",
    "    train_miou = miou_metric.compute()\n",
    "\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = []\n",
    "    saved_outputs = []\n",
    "    saved_labels = []\n",
    "    f1_score_metric.reset()\n",
    "    miou_metric.reset()\n",
    "    progress_bar = tqdm(valid_dataloader, desc=\"Valid\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for imgs, labels in progress_bar:\n",
    "            imgs, labels = imgs.float().to(device), labels.to(device)\n",
    "\n",
    "            output = model(imgs)\n",
    "            loss = criterion(output, labels)\n",
    "\n",
    "\n",
    "            # F1 Score와 MIoU 계산 (검증 단계)\n",
    "            f1_score = f1_score_metric(output, labels)\n",
    "            miou = miou_metric(output, labels)\n",
    "\n",
    "            saved_outputs.append(output)\n",
    "            saved_labels.append(labels)\n",
    "\n",
    "            # tqdm 바 업데이트 (검증 단계 손실, F1 Score, MIoU 포함)\n",
    "            progress_bar.set_postfix(loss=loss.item(), F1_score=f1_score.item(), mIoU=miou.item())\n",
    "            val_loss.append(loss.item())\n",
    "\n",
    "    saved_outputs = torch.cat(saved_outputs, dim=0)\n",
    "    saved_labels = torch.cat(saved_labels, dim=0)\n",
    "    avg_val_loss = sum(val_loss) / len(val_loss)\n",
    "    val_f1 = f1_score_metric.compute()\n",
    "    val_miou = miou_metric.compute()\n",
    "\n",
    "    # epoch 결과 출력\n",
    "    print(f\"Epoch {epoch}/{epochs} - Train Loss: {avg_train_loss:.4f}, Train F1: {train_f1:.4f}, Train MIoU: {train_miou:.4f}, \"\n",
    "            f\"Val Loss: {avg_val_loss:.4f}, Val F1: {val_f1:.4f}, Val MIoU: {val_miou:.4f}\")\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "\n",
    "    f1_score_metric.reset()\n",
    "    miou_metric.reset()\n",
    "\n",
    "    miou_values = []\n",
    "    thresholds = np.arange(-10, 1, 0.1)\n",
    "    miou_tmp = BinaryJaccardIndex().to(device)\n",
    "\n",
    "    for threshold in tqdm(thresholds):\n",
    "        # output을 이진 마스크로 변환\n",
    "        miou_tmp.reset()\n",
    "        saved_outputs_binary = saved_outputs > threshold\n",
    "        miou = miou_tmp(saved_outputs_binary.cuda(), saved_labels)\n",
    "        miou_values.append(miou.item())\n",
    "\n",
    "    # 가장 높은 mIoU 값 찾기\n",
    "    max_miou = max(miou_values)\n",
    "    max_threshold = thresholds[miou_values.index(max_miou)]\n",
    "    # 최적 모델 저장 로직 (예시)\n",
    "    if max_miou > best_val_miou:\n",
    "        best_val_miou = max_miou\n",
    "        torch.save(model, f\"./model_save/{epoch}_jihwan1_{max_miou:.4f}_{max_threshold:.4f}.pth\")\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(thresholds, miou_values, marker='o', linestyle='-', color='b')\n",
    "    plt.title(f'max_threshold: {max_threshold}, max_MIoU: {max_miou}')\n",
    "    plt.xlabel('Threshold')\n",
    "    plt.ylabel('Mean Intersection over Union (MIoU)')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cHb_ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
